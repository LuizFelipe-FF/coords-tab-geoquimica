{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOKbOWKYpfQqxo5x/8Equ2r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuizFelipe-FF/coords-tab-geoquimica/blob/main/padroniza%C3%A7%C3%A3o_coors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "C_jC09UEf2RI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VAGL50Pof78t"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-ADcwEvOhB7t"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lKPyPcZAkDN1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Funcionou pra maioria"
      ],
      "metadata": {
        "id": "-J0ZUiPqCbz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import geopandas as gpd\n",
        "import pyproj\n",
        "from shapely.geometry import Point\n",
        "\n",
        "# 1. Carregamento do CSV\n",
        "df = pd.read_csv(\"pmp_recalculated.csv\", sep=';')\n",
        "\n",
        "# 2. Nomes únicos\n",
        "def make_unique_names(series):\n",
        "    counts = {}\n",
        "    unique = []\n",
        "    for name in series:\n",
        "        if pd.isna(name):\n",
        "            unique.append(name)\n",
        "            continue\n",
        "        count = counts.get(name, 0)\n",
        "        suffix = '' if count == 0 else f'_{chr(96 + count + 1)}'\n",
        "        unique.append(f\"{name}{suffix}\")\n",
        "        counts[name] = count + 1\n",
        "    return unique\n",
        "\n",
        "df['sample_name_unique'] = make_unique_names(df['sample_name'])\n",
        "\n",
        "# 3. Expansão de coordenadas compostas\n",
        "def expand_multiple_coordinates(df):\n",
        "    rows = []\n",
        "    for _, row in df.iterrows():\n",
        "        coord_str = str(row['longitude'])\n",
        "        if 'and' in coord_str and '/' in coord_str:\n",
        "            parts = [pair.strip() for pair in coord_str.split('and')]\n",
        "            for i, pair in enumerate(parts):\n",
        "                try:\n",
        "                    lon_raw, lat_raw = pair.split('/')\n",
        "                    new_row = row.copy()\n",
        "                    new_row['lon_raw'] = lon_raw.strip()\n",
        "                    new_row['lat_raw'] = lat_raw.strip()\n",
        "                    new_row['sample_name_unique'] += f\"_{chr(97 + i)}\"\n",
        "                    rows.append(new_row)\n",
        "                except:\n",
        "                    continue\n",
        "        else:\n",
        "            new_row = row.copy()\n",
        "            new_row['lat_raw'] = str(row['latitude'])\n",
        "            new_row['lon_raw'] = str(row['longitude'])\n",
        "            rows.append(new_row)\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "df_expanded = expand_multiple_coordinates(df)\n",
        "\n",
        "# 4. Limpeza\n",
        "def normalizar(coord):\n",
        "    if pd.isna(coord): return coord\n",
        "    coord = str(coord).strip()\n",
        "    coord = coord.replace(',', '.')\n",
        "    coord = coord.replace('º', '°').replace(\"’\", \"'\").replace(\"‘\", \"'\")\n",
        "    coord = coord.replace(\"″\", '\"').replace(\"”\", '\"').replace(\"“\", '\"')\n",
        "    coord = coord.replace(\"´\", \"'\").replace(\"?\", \"'\").replace(\"Â\", \"\")\n",
        "    coord = coord.replace('N', '').replace('S', '').replace('E', '').replace('W', '')\n",
        "    coord = coord.replace('\\u2013', '-').replace('\\u2014', '-').replace('\\u2212', '-')\n",
        "    coord = coord.replace(' ', '')\n",
        "    if coord.count('.') > 1 and not re.search(r'\\d\\.\\d+$', coord):\n",
        "        coord = coord.replace('.', '')\n",
        "    if re.fullmatch(r'-?\\d{5,}', coord):\n",
        "        coord = re.sub(r'^(-?\\d{2})(\\d+)$', r'\\1.\\2', coord)\n",
        "    return coord\n",
        "\n",
        "df_expanded['lat_raw'] = df_expanded['lat_raw'].apply(normalizar)\n",
        "df_expanded['lon_raw'] = df_expanded['lon_raw'].apply(normalizar)\n",
        "\n",
        "# 5. Funções auxiliares\n",
        "def dms_to_decimal(d, m, s, hemi=''):\n",
        "    val = abs(float(d)) + float(m)/60 + float(s)/3600\n",
        "    return -val if hemi in ['S', 'W'] else val\n",
        "\n",
        "def parse_coord(coord):\n",
        "    try: return float(coord)\n",
        "    except: pass\n",
        "    dms = re.match(r'(-?\\d+)[°]?\\s*(\\d+)?\\'?\\s*(\\d+(?:\\.\\d+)?)?\\\"?\\s*([NSEW])?', coord, re.IGNORECASE)\n",
        "    if dms:\n",
        "        g, m, s, h = dms.groups()\n",
        "        return dms_to_decimal(g, m or 0, s or 0, h.upper() if h else '')\n",
        "    dmm = re.match(r'(-?\\d+)[°]?\\s*(\\d+(?:\\.\\d+)?)[\\'\\s]*([NSEW])?', coord, re.IGNORECASE)\n",
        "    if dmm:\n",
        "        g, m, h = dmm.groups()\n",
        "        return dms_to_decimal(g, m, 0, h.upper() if h else '')\n",
        "    decimal = re.match(r'(-?\\d+(?:\\.\\d+)?)[°]?\\s*([NSEW])', coord, re.IGNORECASE)\n",
        "    if decimal:\n",
        "        val, h = decimal.groups()\n",
        "        val = float(val)\n",
        "        return -abs(val) if h.upper() in ['S', 'W'] else abs(val)\n",
        "    return np.nan\n",
        "\n",
        "def utm_to_latlon(e, n, zone=22, hemisphere='S'):\n",
        "    try:\n",
        "        crs_utm = pyproj.CRS(proj='utm', zone=zone, south=(hemisphere.upper() == 'S'))\n",
        "        transformer = pyproj.Transformer.from_crs(crs_utm, 'EPSG:4326', always_xy=True)\n",
        "        lon, lat = transformer.transform(e, n)\n",
        "        return lat, lon\n",
        "    except:\n",
        "        return np.nan, np.nan\n",
        "\n",
        "def limpar_utm(coord):\n",
        "    if pd.isna(coord): return None\n",
        "    coord = re.sub(r'[^\\d]', '', str(coord))\n",
        "    try: return int(coord)\n",
        "    except: return None\n",
        "\n",
        "def corrigir_decimal_mal_posicionado(coord):\n",
        "    try:\n",
        "        if isinstance(coord, str) and re.fullmatch(r'-\\d{3}\\.\\d+', coord):\n",
        "            numeros = re.sub(r'[^\\d]', '', coord)\n",
        "            if len(numeros) >= 6:\n",
        "                return float(f\"-{numeros[:2]}.{numeros[2:]}\")\n",
        "    except:\n",
        "        pass\n",
        "    return np.nan\n",
        "\n",
        "# 6. Conversão principal\n",
        "def process_coordinates(row):\n",
        "    lat = row['lat_raw']\n",
        "    lon = row['lon_raw']\n",
        "\n",
        "    if isinstance(lon, str) and '/' in lon:\n",
        "        try:\n",
        "            e, n = map(lambda x: int(re.sub(r'\\D', '', x)), lon.split('/'))\n",
        "            return pd.Series(utm_to_latlon(e, n))\n",
        "        except:\n",
        "            return pd.Series([np.nan, np.nan])\n",
        "\n",
        "    try:\n",
        "        lat_dd = parse_coord(lat)\n",
        "        lon_dd = parse_coord(lon)\n",
        "\n",
        "        if pd.isna(lat_dd) or pd.isna(lon_dd):\n",
        "            e_raw = limpar_utm(lon)\n",
        "            n_raw = limpar_utm(lat)\n",
        "            if e_raw and n_raw and 100000 < e_raw < 10000000:\n",
        "                return pd.Series(utm_to_latlon(e_raw, n_raw))\n",
        "        return pd.Series([lat_dd, lon_dd])\n",
        "    except:\n",
        "        return pd.Series([np.nan, np.nan])\n",
        "\n",
        "df_expanded[['lat_dd', 'lon_dd']] = df_expanded.apply(process_coordinates, axis=1)\n",
        "\n",
        "# 7. Recuperar ponto mal posicionado\n",
        "def aplicar_recuperacao(row):\n",
        "    lat, lon = row['lat_dd'], row['lon_dd']\n",
        "    if pd.isna(lat):\n",
        "        lat = corrigir_decimal_mal_posicionado(row['lat_raw'])\n",
        "    if pd.isna(lon):\n",
        "        lon = corrigir_decimal_mal_posicionado(row['lon_raw'])\n",
        "    return pd.Series([lat, lon])\n",
        "\n",
        "df_expanded[['lat_dd', 'lon_dd']] = df_expanded.apply(aplicar_recuperacao, axis=1)\n",
        "\n",
        "# 8. Corrigir sinal\n",
        "def corrigir_sinal(lat, lon):\n",
        "    if pd.isna(lat) or pd.isna(lon): return np.nan, np.nan\n",
        "    try:\n",
        "        lat = float(lat)\n",
        "        lon = float(lon)\n",
        "        if lat > 0: lat *= -1\n",
        "        if lon > 0: lon *= -1\n",
        "        if not (-90 <= lat <= 0 and -180 <= lon <= 0):\n",
        "            return np.nan, np.nan\n",
        "        return lat, lon\n",
        "    except:\n",
        "        return np.nan, np.nan\n",
        "\n",
        "df_expanded[['lat_dd', 'lon_dd']] = df_expanded.apply(\n",
        "    lambda row: pd.Series(corrigir_sinal(row['lat_dd'], row['lon_dd'])),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# 9. Diagnóstico\n",
        "def classificar_falha(row):\n",
        "    if pd.isna(row['lat_raw']) or pd.isna(row['lon_raw']):\n",
        "        return 'coord ausente'\n",
        "    try:\n",
        "        float(row['lat_raw'])\n",
        "        float(row['lon_raw'])\n",
        "    except:\n",
        "        return 'coord mal formatada'\n",
        "    if abs(float(row['lat_raw'])) > 90 or abs(float(row['lon_raw'])) > 180:\n",
        "        return 'grau inválido'\n",
        "    return 'parse falhou'\n",
        "\n",
        "df_expanded['conversao'] = np.where(\n",
        "    df_expanded[['lat_dd', 'lon_dd']].notna().all(axis=1),\n",
        "    'Convertido', 'Falha'\n",
        ")\n",
        "\n",
        "df_expanded['motivo_falha'] = df_expanded.apply(\n",
        "    lambda r: classificar_falha(r) if r['conversao'] == 'Falha' else '',\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# 10. Exportar falhas\n",
        "falhas = df_expanded[df_expanded['conversao'] == 'Falha']\n",
        "falhas.to_csv(\"coordenadas_falhas.csv\", index=False)\n",
        "\n",
        "# 11. Exportar convertidos\n",
        "df_ok = df_expanded[df_expanded['conversao'] == 'Convertido']\n",
        "gdf = gpd.GeoDataFrame(\n",
        "    df_ok,\n",
        "    geometry=gpd.points_from_xy(df_ok['lon_dd'], df_ok['lat_dd']),\n",
        "    crs=\"EPSG:4326\"\n",
        ")\n",
        "gdf.to_file(\"amostras_convertidas.shp\", driver=\"ESRI Shapefile\", encoding=\"utf-8\")\n",
        "gdf.to_csv(\"amostras_convertidas.csv\", sep=';', index=False)\n",
        "\n",
        "# 12. Painel final de status\n",
        "print(\"\\n✅ Painel de Conversão Final\")\n",
        "print(f\"Total original: {df.shape[0]}\")\n",
        "print(f\"Após expansão de coordenadas: {df_expanded.shape[0]}\")\n",
        "print(f\"Coordenadas convertidas com sucesso: {df_ok.shape[0]}\")\n",
        "print(f\"Coordenadas com falha: {falhas.shape[0]}\")\n",
        "print(\"\\n📊 Classificação das falhas:\")\n",
        "print(falhas['motivo_falha'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_dYMDYDmLPt",
        "outputId": "53a74127-362f-41d9-efee-43852071a926"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Painel de Conversão Final\n",
            "Total original: 3742\n",
            "Após expansão de coordenadas: 3769\n",
            "Coordenadas convertidas com sucesso: 2300\n",
            "Coordenadas com falha: 1469\n",
            "\n",
            "📊 Classificação das falhas:\n",
            "motivo_falha\n",
            "parse falhou           1401\n",
            "grau inválido            35\n",
            "coord mal formatada      33\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "diferentão mas nao rodou tao bem"
      ],
      "metadata": {
        "id": "w5YUPwNnsj8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import geopandas as gpd\n",
        "import pyproj\n",
        "from shapely.geometry import Point\n",
        "\n",
        "# ===============================================\n",
        "# 1. Funções de parsing refinadas\n",
        "# ===============================================\n",
        "\n",
        "def normalize_coord_string(coord_str):\n",
        "    if pd.isna(coord_str): return None\n",
        "    coord_str = str(coord_str).strip()\n",
        "    coord_str = coord_str.replace(',', '.')\n",
        "    coord_str = coord_str.replace(\"º\", \"°\").replace(\"’\", \"'\").replace(\"‘\", \"'\")\n",
        "    coord_str = coord_str.replace(\"″\", '\"').replace(\"”\", '\"').replace(\"“\", '\"')\n",
        "    coord_str = coord_str.replace(\"´\", \"'\").replace(\"?\", \"'\").replace(\"Â\", \"\")\n",
        "    coord_str = coord_str.replace('N', '').replace('S', '').replace('E', '').replace('W', '')\n",
        "    coord_str = coord_str.replace('\\u2013', '-').replace('\\u2014', '-').replace('\\u2212', '-')\n",
        "    coord_str = coord_str.replace(' ', '')\n",
        "\n",
        "    if coord_str.count('.') > 1 and not re.search(r'\\d+\\.\\d+$', coord_str):\n",
        "        parts = coord_str.split('.')\n",
        "        coord_str = ''.join(parts[:-1]) + '.' + parts[-1]\n",
        "\n",
        "    if re.fullmatch(r'-?\\d+\\.\\d+\\.\\d+', coord_str):\n",
        "        parts = coord_str.split('.')\n",
        "        coord_str = ''.join(parts[:-1]) + '.' + parts[-1]\n",
        "\n",
        "    return coord_str\n",
        "\n",
        "def dms_to_decimal(degrees, minutes, seconds):\n",
        "    try:\n",
        "        deg = float(degrees)\n",
        "        min = float(minutes)\n",
        "        sec = float(seconds)\n",
        "        val = abs(deg) + (min / 60) + (sec / 3600)\n",
        "        return -val if deg > 0 else deg\n",
        "    except:\n",
        "        return np.nan\n",
        "\n",
        "def dmm_to_decimal(degrees, minutes_decimal):\n",
        "    try:\n",
        "        deg = float(degrees)\n",
        "        min_dec = float(minutes_decimal)\n",
        "        val = abs(deg) + (min_dec / 60)\n",
        "        return -val if deg > 0 else deg\n",
        "    except:\n",
        "        return np.nan\n",
        "\n",
        "def parse_and_convert_coord(coord_str, is_longitude=False):\n",
        "    norm = normalize_coord_string(coord_str)\n",
        "    if norm is None: return np.nan\n",
        "\n",
        "    dms_match = re.match(r'(-?\\d+)[°]?\\s*(\\d+)?\\'?\\s*(\\d+(?:\\.\\d+)?)?\"?', norm)\n",
        "    if dms_match:\n",
        "        g, m, s = dms_match.groups()\n",
        "        return dms_to_decimal(g, m or 0, s or 0)\n",
        "\n",
        "    dmm_match = re.match(r'(-?\\d+)[°]?\\s*(\\d+\\.\\d+)', norm)\n",
        "    if dmm_match:\n",
        "        g, m_dec = dmm_match.groups()\n",
        "        return dmm_to_decimal(g, m_dec)\n",
        "\n",
        "    try:\n",
        "        val = float(norm)\n",
        "        if abs(val) > 10 and abs(val) < 1000 and re.match(r'^-?\\d{3}\\.\\d+$', norm) and not is_longitude:\n",
        "            val = val / 10\n",
        "        return -abs(val)\n",
        "    except:\n",
        "        return np.nan\n",
        "\n",
        "# ===============================================\n",
        "# 2. Etapas do pipeline com pandas\n",
        "# ===============================================\n",
        "\n",
        "df = pd.read_csv(\"pmp_recalculated.csv\", sep=';')\n",
        "\n",
        "def make_unique_names(series):\n",
        "    counts = {}\n",
        "    result = []\n",
        "    for name in series:\n",
        "        if pd.isna(name):\n",
        "            result.append(name)\n",
        "            continue\n",
        "        c = counts.get(name, 0)\n",
        "        suffix = '' if c == 0 else f'_{chr(96 + c + 1)}'\n",
        "        result.append(f\"{name}{suffix}\")\n",
        "        counts[name] = c + 1\n",
        "    return result\n",
        "\n",
        "df['sample_name_unique'] = make_unique_names(df['sample_name'])\n",
        "\n",
        "def expand_multiple_coords(df):\n",
        "    rows = []\n",
        "    for _, row in df.iterrows():\n",
        "        coord_str = str(row['longitude'])\n",
        "        if 'and' in coord_str and '/' in coord_str:\n",
        "            parts = [pair.strip() for pair in coord_str.split('and')]\n",
        "            for i, pair in enumerate(parts):\n",
        "                try:\n",
        "                    lon_raw, lat_raw = pair.split('/')\n",
        "                    new_row = row.copy()\n",
        "                    new_row['lon_raw'] = lon_raw.strip()\n",
        "                    new_row['lat_raw'] = lat_raw.strip()\n",
        "                    new_row['sample_name_unique'] += f\"_{chr(97 + i)}\"\n",
        "                    rows.append(new_row)\n",
        "                except:\n",
        "                    continue\n",
        "        else:\n",
        "            new_row = row.copy()\n",
        "            new_row['lat_raw'] = str(row['latitude'])\n",
        "            new_row['lon_raw'] = str(row['longitude'])\n",
        "            rows.append(new_row)\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "df_expanded = expand_multiple_coords(df)\n",
        "\n",
        "# ===============================================\n",
        "# 3. Conversão usando o parser refinado\n",
        "# ===============================================\n",
        "\n",
        "def parse_row(row):\n",
        "    lat = parse_and_convert_coord(row['lat_raw'], is_longitude=False)\n",
        "    lon = parse_and_convert_coord(row['lon_raw'], is_longitude=True)\n",
        "    return pd.Series([lat, lon])\n",
        "\n",
        "df_expanded[['lat_dd', 'lon_dd']] = df_expanded.apply(parse_row, axis=1)\n",
        "\n",
        "# 4. Correção de sinal e faixa\n",
        "def corrigir_sinal(lat, lon):\n",
        "    if pd.isna(lat) or pd.isna(lon): return np.nan, np.nan\n",
        "    if lat > 0: lat *= -1\n",
        "    if lon > 0: lon *= -1\n",
        "    if not (-90 <= lat <= 0 and -180 <= lon <= 0):\n",
        "        return np.nan, np.nan\n",
        "    return lat, lon\n",
        "\n",
        "df_expanded[['lat_dd', 'lon_dd']] = df_expanded.apply(\n",
        "    lambda r: pd.Series(corrigir_sinal(r['lat_dd'], r['lon_dd'])),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# 5. Diagnóstico\n",
        "def diagnostico(row):\n",
        "    if pd.isna(row['lat_raw']) or pd.isna(row['lon_raw']):\n",
        "        return 'coord ausente'\n",
        "    try:\n",
        "        float(row['lat_raw'])\n",
        "        float(row['lon_raw'])\n",
        "    except:\n",
        "        return 'coord mal formatada'\n",
        "    if abs(float(row['lat_raw'])) > 90 or abs(float(row['lon_raw'])) > 180:\n",
        "        return 'grau inválido'\n",
        "    return 'parse falhou'\n",
        "\n",
        "df_expanded['conversao'] = np.where(\n",
        "    df_expanded[['lat_dd', 'lon_dd']].notna().all(axis=1), 'Convertido', 'Falha'\n",
        ")\n",
        "df_expanded['motivo_falha'] = df_expanded.apply(\n",
        "    lambda r: diagnostico(r) if r['conversao'] == 'Falha' else '',\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# 6. Exportações\n",
        "df_expanded[df_expanded['conversao'] == 'Falha'].to_csv(\"coordenadas_falhas.csv\", index=False)\n",
        "\n",
        "df_ok = df_expanded[df_expanded['conversao'] == 'Convertido'].copy()\n",
        "\n",
        "gdf = gpd.GeoDataFrame(\n",
        "    df_ok,\n",
        "    geometry=gpd.points_from_xy(df_ok['lon_dd'], df_ok['lat_dd']),\n",
        "    crs=\"EPSG:4326\"\n",
        ")\n",
        "\n",
        "gdf.to_file(\"amostras_convertidas.shp\", driver=\"ESRI Shapefile\", encoding=\"utf-8\")\n",
        "gdf.to_csv(\"amostras_convertidas.csv\", sep=';', index=False)\n",
        "\n",
        "# 7. Painel\n",
        "print(\"\\n✅ Conversão concluída\")\n",
        "print(f\"Original: {df.shape[0]}\")\n",
        "print(f\"Após expansão: {df_expanded.shape[0]}\")\n",
        "print(f\"Convertidas: {(df_expanded['conversao'] == 'Convertido').sum()}\")\n",
        "print(f\"Falhas: {(df_expanded['conversao'] == 'Falha').sum()}\")\n",
        "print(\"\\n📊 Motivos das falhas:\")\n",
        "print(df_expanded['motivo_falha'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15UWjafhoIU4",
        "outputId": "49ad7477-6c5a-45dd-c3e7-7e25746c1a94"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Conversão concluída\n",
            "Original: 3742\n",
            "Após expansão: 3769\n",
            "Convertidas: 1943\n",
            "Falhas: 1826\n",
            "\n",
            "📊 Motivos das falhas:\n",
            "motivo_falha\n",
            "                       1943\n",
            "parse falhou           1401\n",
            "grau inválido           361\n",
            "coord mal formatada      64\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j8ZCpOI4sls1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}