{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9xeA6umpPDoGbmhSvSrwy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuizFelipe-FF/coords-tab-geoquimica/blob/main/padroniza%C3%A7%C3%A3o_coors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "___\n",
        "*Autor: Luiz Felipe Franco Ferreira - Graduando em Geologia*\n",
        "\n",
        "*Atividade: Seleção de dados e criação de um SHP*\n",
        "\n",
        "*Data: 18/06/2025*\n",
        "\n",
        "*Resumo: Código desenvolvido para garantir ID's únicos para cada amostra, padronizar os diversos tipos de coordenadas,criação de um Geodataframe agrupando as amostras que possuam Lat e Long, e gerar um SHP.*\n",
        "___"
      ],
      "metadata": {
        "id": "osBGo6p_2Ew4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Funcionou pra maioria"
      ],
      "metadata": {
        "id": "-J0ZUiPqCbz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import geopandas as gpd\n",
        "import pyproj\n",
        "from shapely.geometry import Point\n",
        "\n",
        "# 1. Carregamento do CSV\n",
        "df = pd.read_csv(\"pmp_recalculated.csv\", sep=';')\n",
        "\n",
        "# 2. Nomes únicos\n",
        "def make_unique_names(series):\n",
        "    counts = {}\n",
        "    unique = []\n",
        "    for name in series:\n",
        "        if pd.isna(name):\n",
        "            unique.append(name)\n",
        "            continue\n",
        "        count = counts.get(name, 0)\n",
        "        suffix = '' if count == 0 else f'_{chr(96 + count + 1)}'\n",
        "        unique.append(f\"{name}{suffix}\")\n",
        "        counts[name] = count + 1\n",
        "    return unique\n",
        "\n",
        "df['sample_name_unique'] = make_unique_names(df['sample_name'])\n",
        "\n",
        "# 3. Expansão de coordenadas compostas\n",
        "def expand_multiple_coordinates(df):\n",
        "    rows = []\n",
        "    for _, row in df.iterrows():\n",
        "        coord_str = str(row['longitude'])\n",
        "        if 'and' in coord_str and '/' in coord_str:\n",
        "            parts = [pair.strip() for pair in coord_str.split('and')]\n",
        "            for i, pair in enumerate(parts):\n",
        "                try:\n",
        "                    lon_raw, lat_raw = pair.split('/')\n",
        "                    new_row = row.copy()\n",
        "                    new_row['lon_raw'] = lon_raw.strip()\n",
        "                    new_row['lat_raw'] = lat_raw.strip()\n",
        "                    new_row['sample_name_unique'] += f\"_{chr(97 + i)}\"\n",
        "                    rows.append(new_row)\n",
        "                except:\n",
        "                    continue\n",
        "        else:\n",
        "            new_row = row.copy()\n",
        "            new_row['lat_raw'] = str(row['latitude'])\n",
        "            new_row['lon_raw'] = str(row['longitude'])\n",
        "            rows.append(new_row)\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "df_expanded = expand_multiple_coordinates(df)\n",
        "\n",
        "# 4. Limpeza\n",
        "def normalizar(coord):\n",
        "    if pd.isna(coord): return coord\n",
        "    coord = str(coord).strip()\n",
        "    coord = coord.replace(',', '.')\n",
        "    coord = coord.replace('º', '°').replace(\"’\", \"'\").replace(\"‘\", \"'\")\n",
        "    coord = coord.replace(\"″\", '\"').replace(\"”\", '\"').replace(\"“\", '\"')\n",
        "    coord = coord.replace(\"´\", \"'\").replace(\"?\", \"'\").replace(\"Â\", \"\")\n",
        "    coord = coord.replace('N', '').replace('S', '').replace('E', '').replace('W', '')\n",
        "    coord = coord.replace('\\u2013', '-').replace('\\u2014', '-').replace('\\u2212', '-')\n",
        "    coord = coord.replace(' ', '')\n",
        "    if coord.count('.') > 1 and not re.search(r'\\d\\.\\d+$', coord):\n",
        "        coord = coord.replace('.', '')\n",
        "    if re.fullmatch(r'-?\\d{5,}', coord):\n",
        "        coord = re.sub(r'^(-?\\d{2})(\\d+)$', r'\\1.\\2', coord)\n",
        "    return coord\n",
        "\n",
        "df_expanded['lat_raw'] = df_expanded['lat_raw'].apply(normalizar)\n",
        "df_expanded['lon_raw'] = df_expanded['lon_raw'].apply(normalizar)\n",
        "\n",
        "# 5. Funções auxiliares\n",
        "def dms_to_decimal(d, m, s, hemi=''):\n",
        "    val = abs(float(d)) + float(m)/60 + float(s)/3600\n",
        "    return -val if hemi in ['S', 'W'] else val\n",
        "\n",
        "def parse_coord(coord):\n",
        "    try: return float(coord)\n",
        "    except: pass\n",
        "    dms = re.match(r'(-?\\d+)[°]?\\s*(\\d+)?\\'?\\s*(\\d+(?:\\.\\d+)?)?\\\"?\\s*([NSEW])?', coord, re.IGNORECASE)\n",
        "    if dms:\n",
        "        g, m, s, h = dms.groups()\n",
        "        return dms_to_decimal(g, m or 0, s or 0, h.upper() if h else '')\n",
        "    dmm = re.match(r'(-?\\d+)[°]?\\s*(\\d+(?:\\.\\d+)?)[\\'\\s]*([NSEW])?', coord, re.IGNORECASE)\n",
        "    if dmm:\n",
        "        g, m, h = dmm.groups()\n",
        "        return dms_to_decimal(g, m, 0, h.upper() if h else '')\n",
        "    decimal = re.match(r'(-?\\d+(?:\\.\\d+)?)[°]?\\s*([NSEW])', coord, re.IGNORECASE)\n",
        "    if decimal:\n",
        "        val, h = decimal.groups()\n",
        "        val = float(val)\n",
        "        return -abs(val) if h.upper() in ['S', 'W'] else abs(val)\n",
        "    return np.nan\n",
        "\n",
        "def utm_to_latlon(e, n, zone=22, hemisphere='S'):\n",
        "    try:\n",
        "        crs_utm = pyproj.CRS(proj='utm', zone=zone, south=(hemisphere.upper() == 'S'))\n",
        "        transformer = pyproj.Transformer.from_crs(crs_utm, 'EPSG:4326', always_xy=True)\n",
        "        lon, lat = transformer.transform(e, n)\n",
        "        return lat, lon\n",
        "    except:\n",
        "        return np.nan, np.nan\n",
        "\n",
        "def limpar_utm(coord):\n",
        "    if pd.isna(coord): return None\n",
        "    coord = re.sub(r'[^\\d]', '', str(coord))\n",
        "    try: return int(coord)\n",
        "    except: return None\n",
        "\n",
        "def corrigir_decimal_mal_posicionado(coord):\n",
        "    try:\n",
        "        if isinstance(coord, str) and re.fullmatch(r'-\\d{3}\\.\\d+', coord):\n",
        "            numeros = re.sub(r'[^\\d]', '', coord)\n",
        "            if len(numeros) >= 6:\n",
        "                return float(f\"-{numeros[:2]}.{numeros[2:]}\")\n",
        "    except:\n",
        "        pass\n",
        "    return np.nan\n",
        "\n",
        "# 6. Conversão principal\n",
        "def process_coordinates(row):\n",
        "    lat = row['lat_raw']\n",
        "    lon = row['lon_raw']\n",
        "\n",
        "    if isinstance(lon, str) and '/' in lon:\n",
        "        try:\n",
        "            e, n = map(lambda x: int(re.sub(r'\\D', '', x)), lon.split('/'))\n",
        "            return pd.Series(utm_to_latlon(e, n))\n",
        "        except:\n",
        "            return pd.Series([np.nan, np.nan])\n",
        "\n",
        "    try:\n",
        "        lat_dd = parse_coord(lat)\n",
        "        lon_dd = parse_coord(lon)\n",
        "\n",
        "        if pd.isna(lat_dd) or pd.isna(lon_dd):\n",
        "            e_raw = limpar_utm(lon)\n",
        "            n_raw = limpar_utm(lat)\n",
        "            if e_raw and n_raw and 100000 < e_raw < 10000000:\n",
        "                return pd.Series(utm_to_latlon(e_raw, n_raw))\n",
        "        return pd.Series([lat_dd, lon_dd])\n",
        "    except:\n",
        "        return pd.Series([np.nan, np.nan])\n",
        "\n",
        "df_expanded[['lat_dd', 'lon_dd']] = df_expanded.apply(process_coordinates, axis=1)\n",
        "\n",
        "# 7. Recuperar ponto mal posicionado\n",
        "def aplicar_recuperacao(row):\n",
        "    lat, lon = row['lat_dd'], row['lon_dd']\n",
        "    if pd.isna(lat):\n",
        "        lat = corrigir_decimal_mal_posicionado(row['lat_raw'])\n",
        "    if pd.isna(lon):\n",
        "        lon = corrigir_decimal_mal_posicionado(row['lon_raw'])\n",
        "    return pd.Series([lat, lon])\n",
        "\n",
        "df_expanded[['lat_dd', 'lon_dd']] = df_expanded.apply(aplicar_recuperacao, axis=1)\n",
        "\n",
        "# 8. Corrigir sinal\n",
        "def corrigir_sinal(lat, lon):\n",
        "    if pd.isna(lat) or pd.isna(lon): return np.nan, np.nan\n",
        "    try:\n",
        "        lat = float(lat)\n",
        "        lon = float(lon)\n",
        "        if lat > 0: lat *= -1\n",
        "        if lon > 0: lon *= -1\n",
        "        if not (-90 <= lat <= 0 and -180 <= lon <= 0):\n",
        "            return np.nan, np.nan\n",
        "        return lat, lon\n",
        "    except:\n",
        "        return np.nan, np.nan\n",
        "\n",
        "df_expanded[['lat_dd', 'lon_dd']] = df_expanded.apply(\n",
        "    lambda row: pd.Series(corrigir_sinal(row['lat_dd'], row['lon_dd'])),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# 9. Diagnóstico\n",
        "def classificar_falha(row):\n",
        "    if pd.isna(row['lat_raw']) or pd.isna(row['lon_raw']):\n",
        "        return 'coord ausente'\n",
        "    try:\n",
        "        float(row['lat_raw'])\n",
        "        float(row['lon_raw'])\n",
        "    except:\n",
        "        return 'coord mal formatada'\n",
        "    if abs(float(row['lat_raw'])) > 90 or abs(float(row['lon_raw'])) > 180:\n",
        "        return 'grau inválido'\n",
        "    return 'parse falhou'\n",
        "\n",
        "df_expanded['conversao'] = np.where(\n",
        "    df_expanded[['lat_dd', 'lon_dd']].notna().all(axis=1),\n",
        "    'Convertido', 'Falha'\n",
        ")\n",
        "\n",
        "df_expanded['motivo_falha'] = df_expanded.apply(\n",
        "    lambda r: classificar_falha(r) if r['conversao'] == 'Falha' else '',\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# 10. Exportar falhas\n",
        "falhas = df_expanded[df_expanded['conversao'] == 'Falha']\n",
        "falhas.to_csv(\"coordenadas_falhas.csv\", index=False)\n",
        "\n",
        "# 11. Exportar convertidos\n",
        "df_ok = df_expanded[df_expanded['conversao'] == 'Convertido']\n",
        "gdf = gpd.GeoDataFrame(\n",
        "    df_ok,\n",
        "    geometry=gpd.points_from_xy(df_ok['lon_dd'], df_ok['lat_dd']),\n",
        "    crs=\"EPSG:4326\"\n",
        ")\n",
        "\n",
        "# 12. Renomear colunas para o padrão do Shapefile (máx. 10 caracteres)\n",
        "colunas_renomear = {\n",
        "    'sample_name': 'samp_name',\n",
        "    'suite_by_autor': 'suite_aut',\n",
        "    'classification_peate92': 'classif92',\n",
        "    'classification_peate97': 'classif97',\n",
        "    'localization': 'localiz',\n",
        "    '87Sr/86Sr_m': 'Sr86_m',\n",
        "    '87Sr/86Sr_r': 'Sr86_r',\n",
        "    '143Nd/144Nd_m': 'Nd144_m',\n",
        "    '143Nd/144Nd_r': 'Nd144_r',\n",
        "    '206Pb/204Pb_m': 'Pb204_m',\n",
        "    '206Pb/204Pb_r': 'Pb204_r',\n",
        "    '207Pb/204Pb_m': 'Pb207_m',\n",
        "    '207Pb/204Pb_r': 'Pb207_r',\n",
        "    '208Pb/204Pb_m': 'Pb208_m',\n",
        "    '208Pb/204Pb_r': 'Pb208_r',\n",
        "    'sample_name_unique': 'samp_n_unic',\n",
        "    'motivo_falha': 'falha'\n",
        "}\n",
        "\n",
        "# 13. Aplicar renomeação nas colunas do GeoDataFrame\n",
        "gdf.rename(columns=colunas_renomear, inplace=True)\n",
        "\n",
        "gdf.to_file(\"amostras_convertidas.shp\", driver=\"ESRI Shapefile\", encoding=\"utf-8\")\n",
        "gdf.to_csv(\"amostras_convertidas.csv\", sep=';', index=False)\n",
        "\n",
        "# 14. Painel final de status\n",
        "print(\"\\n✅ Painel de Conversão Final\")\n",
        "print(f\"Total original: {df.shape[0]}\")\n",
        "print(f\"Após expansão de coordenadas: {df_expanded.shape[0]}\")\n",
        "print(f\"Coordenadas convertidas com sucesso: {df_ok.shape[0]}\")\n",
        "print(f\"Coordenadas com falha: {falhas.shape[0]}\")\n",
        "print(\"\\n📊 Classificação das falhas:\")\n",
        "print(falhas['motivo_falha'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_dYMDYDmLPt",
        "outputId": "e47c4ea7-43c8-42bc-e310-4c84813ae238"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Painel de Conversão Final\n",
            "Total original: 3742\n",
            "Após expansão de coordenadas: 3769\n",
            "Coordenadas convertidas com sucesso: 2300\n",
            "Coordenadas com falha: 1469\n",
            "\n",
            "📊 Classificação das falhas:\n",
            "motivo_falha\n",
            "parse falhou           1401\n",
            "grau inválido            35\n",
            "coord mal formatada      33\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "C_jC09UEf2RI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VAGL50Pof78t"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-ADcwEvOhB7t"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lKPyPcZAkDN1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "diferentão mas nao rodou tao bem"
      ],
      "metadata": {
        "id": "w5YUPwNnsj8o"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "15UWjafhoIU4"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j8ZCpOI4sls1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}